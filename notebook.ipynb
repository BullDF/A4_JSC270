{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrKKUxrAs0XD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BullDF/A4_JSC270/blob/main/notebook.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BbZMtljs0XF"
      },
      "source": [
        "# JSC270 Assignment 4 by Johnny and Ethan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Rucah_-s0XF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc515e00-5f62-491f-920d-29abebafff0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Import necessary libraries\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUt_PlDhs0XG"
      },
      "source": [
        "## Part I: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPAEGWTos0XG"
      },
      "source": [
        "**a) Consider the training data. What is the balance between the three classes? In other words, what proportion of the observations (in the training set) belong to each class?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eGsSIvKBs0XH",
        "outputId": "4bddc59f-75d3-49c4-b1df-8b5032f56406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   UserName  ScreenName   Location     TweetAt  \\\n",
              "0      3799       48751     London  16-03-2020   \n",
              "1      3800       48752         UK  16-03-2020   \n",
              "2      3801       48753  Vagabonds  16-03-2020   \n",
              "3      3802       48754        NaN  16-03-2020   \n",
              "4      3803       48755        NaN  16-03-2020   \n",
              "\n",
              "                                       OriginalTweet           Sentiment  \n",
              "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
              "1  advice Talk to your neighbours family to excha...            Positive  \n",
              "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
              "3  My food stock is not the only one which is emp...            Positive  \n",
              "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48ebe418-120e-4912-a914-027f3d0e187d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3799</td>\n",
              "      <td>48751</td>\n",
              "      <td>London</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3800</td>\n",
              "      <td>48752</td>\n",
              "      <td>UK</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>advice Talk to your neighbours family to excha...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3801</td>\n",
              "      <td>48753</td>\n",
              "      <td>Vagabonds</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3802</td>\n",
              "      <td>48754</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>My food stock is not the only one which is emp...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3803</td>\n",
              "      <td>48755</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48ebe418-120e-4912-a914-027f3d0e187d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48ebe418-120e-4912-a914-027f3d0e187d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48ebe418-120e-4912-a914-027f3d0e187d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "train_url = 'https://raw.githubusercontent.com/3th4novo/JSC270_a4_dataset/main/Corona_NLP_train.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/3th4novo/JSC270_a4_dataset/main/Corona_NLP_test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_url, encoding='latin-1')\n",
        "df_test = pd.read_csv(test_url, encoding='latin-1')\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8_MNwmls0XH",
        "outputId": "d8aba44f-9b0f-49cf-cec9-480dbc28631b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Positive    18046\n",
              "Negative    15398\n",
              "Neutral      7713\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['Sentiment'].replace({'Extremely Negative': 'Negative', 'Extremely Positive': 'Positive'}, inplace=True)\n",
        "df_train['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_rkgfGSs0XI",
        "outputId": "d4f26ec1-85d5-4d86-bad4-8bd1a8ef2b33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41157"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_train['Sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIAiVmo0s0XI"
      },
      "source": [
        "**b) Tokenize the tweets. In other words, for each observation, convert the tweet from a single string of running text into a list of individual tokens (possibly with punctuation), splitting on whitespace. The result should be that each observation (tweet) is a list of individual tokens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D1RueAbSs0XI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872a807a-25df-4840-d764-3f889a7c5295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [@MeNyrbie, @Phil_Gahan, @Chrisitv, https://t....\n",
            "1    [advice, Talk, to, your, neighbours, family, t...\n",
            "2    [Coronavirus, Australia:, Woolworths, to, give...\n",
            "3    [My, food, stock, is, not, the, only, one, whi...\n",
            "4    [Me,, ready, to, go, at, supermarket, during, ...\n",
            "Name: tokens, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def tokenize(tweet):\n",
        "   return tweet.split()\n",
        "\n",
        "df_train['tokens'] = df_train['OriginalTweet'].apply(tokenize)\n",
        "\n",
        "print(df_train['tokens'].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ-0XdB7s0XI"
      },
      "source": [
        "**c) Using a regular expression, remove any URL tokens from each of the observations.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(lst):\n",
        "  result = [item for item in lst if not re.search('http\\S+|www\\.\\S+', item)]\n",
        "  return result\n",
        "\n",
        "df_train['tokens_no_url'] = df_train['tokens'].apply(remove_url)\n",
        "print(df_train['tokens_no_url'].head(5))"
      ],
      "metadata": {
        "id": "qjMWANAhiC8x",
        "outputId": "aff0f0be-f96d-451c-fbe8-84764ef6ab9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        [@MeNyrbie, @Phil_Gahan, @Chrisitv, and, and]\n",
            "1    [advice, Talk, to, your, neighbours, family, t...\n",
            "2    [Coronavirus, Australia:, Woolworths, to, give...\n",
            "3    [My, food, stock, is, not, the, only, one, whi...\n",
            "4    [Me,, ready, to, go, at, supermarket, during, ...\n",
            "Name: tokens_no_url, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE7XjM2Xs0XI"
      },
      "source": [
        "**d) Remove all punctuation (,.?!;:’\") and special characters(@, #, +, &, =, $, etc). Also, convert all tokens to lowercase only. Can you think of a scenario when you might want to keep some forms of punctuation?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc_spec_lower(lst):\n",
        "    result = [re.sub('[^\\w\\s]', '', item) for item in lst]\n",
        "    lowered = [item.lower() for item in result]\n",
        "    return lowered\n",
        "\n",
        "df_train['tokens_no_punc'] = df_train['tokens_no_url'].apply(remove_punc_spec_lower)\n",
        "print(df_train['tokens_no_punc'].head(5))"
      ],
      "metadata": {
        "id": "qBNZKlGnjbHe",
        "outputId": "887a150b-2958-4aea-eaeb-b48bcefa2f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0           [menyrbie, phil_gahan, chrisitv, and, and]\n",
            "1    [advice, talk, to, your, neighbours, family, t...\n",
            "2    [coronavirus, australia, woolworths, to, give,...\n",
            "3    [my, food, stock, is, not, the, only, one, whi...\n",
            "4    [me, ready, to, go, at, supermarket, during, t...\n",
            "Name: tokens_no_punc, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPwFf6i6s0XI"
      },
      "source": [
        "**e) Now stem your tokens. This will have the effect of converting similar word forms into identical tokens (e.g. run, runs, running → run). Please specify which stemmer you use.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(lst):\n",
        "  token_stems = [stemmer.stem(w) for w in lst]\n",
        "  return token_stems\n",
        "\n",
        "df_train['tokens_stemmed'] = df_train['tokens_no_punc'].apply(stemming)\n",
        "print(df_train['tokens_stemmed'].head(5))"
      ],
      "metadata": {
        "id": "zNWp4-2jkTNu",
        "outputId": "3d69a01c-68a0-4e68-c084-92b24edf43f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0            [menyrbi, phil_gahan, chrisitv, and, and]\n",
            "1    [advic, talk, to, your, neighbour, famili, to,...\n",
            "2    [coronaviru, australia, woolworth, to, give, e...\n",
            "3    [my, food, stock, is, not, the, onli, one, whi...\n",
            "4    [me, readi, to, go, at, supermarket, dure, the...\n",
            "Name: tokens_stemmed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rZnBRP9s0XJ"
      },
      "source": [
        "**f) Lastly, remove stopwords. Using the english stopwords list from nltk, remove these common words from your observations. This list is very long (I think almost 200 words), so remove only the first 100 stopwords in the list.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stopwords.words('english')[0:99]\n",
        "\n",
        "def remove_sw(lst):\n",
        "  tokens_no_sw = []\n",
        "  tokens_no_sw.append([w for w in lst if w not in sw])\n",
        "  return tokens_no_sw\n",
        "\n",
        "df_train['stems_no_sw'] = df_train['tokens_stemmed'].apply(remove_sw)\n",
        "print(df_train['stems_no_sw'].head(5))"
      ],
      "metadata": {
        "id": "KBVGEv9jk7rz",
        "outputId": "4fe61cae-4ac9-4187-aa1e-a571c235446d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                    [[menyrbi, phil_gahan, chrisitv]]\n",
            "1    [[advic, talk, neighbour, famili, exchang, pho...\n",
            "2    [[coronaviru, australia, woolworth, give, elde...\n",
            "3    [[food, stock, not, onli, one, empti, pleas, d...\n",
            "4    [[readi, go, supermarket, dure, covid19, outbr...\n",
            "Name: stems_no_sw, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aps0_7FYs0XJ"
      },
      "source": [
        "**g) Now convert your lists of words into vectors of word counts. You may find Scikit-learn’s CountVectorizer useful here. What is the length of your vocabulary?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEuNvpJZs0XJ"
      },
      "source": [
        "**h) Fit a Naive Bayes model to your data. Report the training and test error of the model. Use accuracy as the error metric. Also, report the 5 most probable words in each class, along with their counts. You might find Scikit-learn’s MultinomialNB() transformer useful. Use Laplace smoothing to prevent probabilities of zero.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sofLx_Res0XJ"
      },
      "source": [
        "**i) Would it be appropriate to fit an ROC curve in this scenario? If yes, explain why. If no, explain why not.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJcLH8Mrs0XJ"
      },
      "source": [
        "**j) Redo parts G-H using TF-IDF vectors instead of count vectors. You might find Scikitlearn’s TfidfVectorizer() transformer useful. Report the training and test accuracy. How does this compare to the accuracy using count vectors?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7z1vqXs0XJ"
      },
      "source": [
        "**k) Recall lemmatization converts each word to its base form, which is a bit stronger than simply taking the stem. Redo parts E-H using TF-IDF vectors instead of count vectors. This time use lemmatization instead of stemming. Report train and test accuracy. How does the accuracy with lemmatization compare to the accuracy with stemming?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYEemL8-s0XJ"
      },
      "source": [
        "**Bonus: Is the Naive Bayes model generative or discriminative? Explain your response.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}